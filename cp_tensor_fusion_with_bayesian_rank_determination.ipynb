{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68acd2f-91f0-43ea-ade5-9981a5817c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import tltorch\n",
    "\n",
    "DTYPE = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7b1ba-33cf-43c6-8b8b-99262af7e878",
   "metadata": {},
   "source": [
    "A PyTorch implementation of CP tensor fusion layer looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "321e4f77-9555-41c0-8e67-572031a24cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowRankFusion(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sizes, output_size, rank, device=None, dtype=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        tensorized_shape = input_sizes + (output_size,)\n",
    "        \n",
    "        self.weight_tensor = tltorch.TensorizedTensor.new(tensorized_shape, \n",
    "                                                          rank, \n",
    "                                                          factorization='CP',\n",
    "                                                          device=device,\n",
    "                                                          dtype=DTYPE)\n",
    "        tltorch.tensor_init(self.weight_tensor)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        output = 1.0\n",
    "        for x, factor in zip(inputs, self.weight_tensor.factors[:-1]):\n",
    "            output = output * (x @ factor)\n",
    "        \n",
    "        output = output @ self.weight_tensor.factors[-1].T\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a687f21c-9483-4481-9083-3bc90365ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sizes = (16, 32, 64)\n",
    "output_size = 32\n",
    "rank = 10\n",
    "fusion_layer = LowRankFusion(input_sizes, output_size, rank, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9620f376-bc08-45c8-8402-bdde6912aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "inputs = [torch.randn((batch_size, input_size), dtype=DTYPE) for input_size in input_sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ad4a22a-9a44-4106-9dd1-73791903422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = fusion_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6de8b576-8876-4887-b4da-49a81cc4b725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50b9b9-85bc-486c-b0c9-d44c99de6264",
   "metadata": {},
   "source": [
    "We need to specify the rank but it is NP hard to determine the rank of a tensor.\n",
    "\n",
    "\n",
    "We can use the bayesian rank determination algorithm from https://arxiv.org/abs/2010.08689 to determine the rank during the training.\n",
    "\n",
    "The goal of bayesian training is to maximize the posterior distribution $P(\\theta|\\mathcal{D}) = \\frac{P(\\mathcal{D}|\\theta)P(\\theta)}{P(\\mathcal{D})}$ where $\\theta$ is parameters and $\\mathcal{D}$ is a dataset and $P(\\mathcal{D}|\\theta)$ is a likelihood function and $P(\\theta)$ is a parameter prior distribution.\n",
    "\n",
    "Since the denominator is a constant, we can maximize the numerator and get the same optimal $\\theta$.\n",
    "\n",
    "Let's look into the parameters of a `AdaptiveRankFusion` layer.\n",
    "\n",
    "A fusion weight is decomposed into CP factors $\\Phi = [U^{(1)}, U^{(2)}, ..., U^{(M)}, U^{(out)}]$ where $U^{(m)} \\in \\mathbb{R}^{s_m \\times R}$ and $U^{(out)} \\in \\mathbb{R}^{s_{out} \\times R}$ and $s_d$ is `input_sizes`[m] and $s_{out}$ is the `output_size` and $R$ is the `max_rank`.\n",
    "\n",
    "We also have a `rank_parameter` $\\lambda \\in \\mathbb{R}^{R}$ that learns the standard deviation of each columns in every factor.\n",
    "\n",
    "Therefore, $P(\\theta)=P(\\lambda)P(\\Phi)=\\prod_{r=1}^R P(\\lambda_r) \\prod_{m=1}^{M+1} \\prod_{i,j} P(U^{(m)}[i,j])$. \n",
    "\n",
    "In theory, we can reduce the rank by 1 if we push the elements of $r$th columns in every factor close to zero.\n",
    "\n",
    "Since our goal is to reduce the `max_rank`, we set a `rank_prior` distribution that prefers values close to 0 such as HalfCauchy(0, $\\eta$) or LogUniform(0,$\\infty$).\n",
    "\n",
    "In other words, $P(\\lambda_r)$ gets higher as $\\lambda_j$ is closer to 0 and higher when $\\lambda_j$ is further from 0. \n",
    "\n",
    "Finally, we set a `factor_prior` distribution as Normal(0,$\\lambda$) so that small $\\lambda_r$ penalizes large values in the $r$th columns of every factor.  \n",
    "\n",
    "During actual training, we maximize $\\log[P(\\theta|\\mathcal{D})] = \\log[P(\\mathcal{D}|\\theta)P(\\theta)] = \\log[P(\\mathcal{D}|\\theta)]+\\log[P(\\theta)]$  \n",
    "\n",
    "So, we compute the `log_prior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "345e6b42-1906-4552-a682-626439839272",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LowRankFusion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xr/p4pqzm995nbcl9w6d8lrb5zh0000gn/T/ipykernel_15820/152710761.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAdaptiveRankFusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLowRankFusion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LowRankFusion' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.distributions.half_cauchy import HalfCauchy\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "class AdaptiveRankFusion(LowRankFusion):\n",
    "    \n",
    "    def __init__(self, input_sizes, output_size, max_rank, eta=0.01, device=None, dtype=None):\n",
    "        \n",
    "        super().__init__(input_sizes, output_size, max_rank, device, dtype)\n",
    "        \n",
    "        self.rank_parameter = nn.Parameter(torch.rand((max_rank,), device=device, dtype=dtype))\n",
    "        self.rank_prior = HalfCauchy(eta)\n",
    "        \n",
    "    def get_log_prior(self):\n",
    "        \n",
    "        clamped_rank_parameter = self.rank_parameter.clamp(1e-5)\n",
    "        self.rank_parameter.data = clamped_rank_parameter.data\n",
    "        \n",
    "        log_prior = 0\n",
    "        log_prior = log_prior + torch.sum(self.rank_prior.log_prob(self.rank_parameter))\n",
    "        \n",
    "        factor_prior = Normal(0, self.rank_parameter)\n",
    "        for factor in self.weight_tensor.factors:\n",
    "            log_prior = log_prior + torch.sum(factor_prior.log_prob(factor))\n",
    "        \n",
    "        return log_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf1589-245d-4a90-a4cb-2a898a660615",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_layer = AdaptiveRankFusion(input_sizes, output_size, max_rank=rank, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb1d64-b08b-4317-bd70-afcbfa177d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_layer.get_log_prior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd81d10-11ea-4c2b-9297-4807b448929f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
