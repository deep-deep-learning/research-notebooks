{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24b3898-c7d8-499c-ada1-7b07cf2fdae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keonyonglee/Projects/torch/tltorch/factorized_tensors/core.py:145: UserWarning: Creating a subclass of FactorizedTensor TensorizedTensor with no name.\n",
      "  warnings.warn(f'Creating a subclass of FactorizedTensor {cls.__name__} with no name.')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tltorch\n",
    "\n",
    "DTYPE = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a6f20-1937-4172-a9d1-a4c14b68b09c",
   "metadata": {},
   "source": [
    "## Tensor Fusion\n",
    "\n",
    "Typically, when inputs from multiple sources (**multimodal inputs**) are given, we concatenate them and feed it through a linear layer to get a single vector. This is easy but not intuitive compared to how humans treat information from different sources. We consider relationship between multimodal information to make our decision, but concatenation fails to capture the intermodal interaction. Therefore, in https://arxiv.org/abs/1707.07250, the authors developed the idea of tensor fusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d0013-f167-4933-93c4-ba616ff04e43",
   "metadata": {},
   "source": [
    "In `tensor_fusion_layer`, the **multimodal inputs** are concatenated with 1 and then outer producted together into a `fusion_tensor`:\n",
    "\n",
    "$\\mathcal{Z} = \\bigotimes_{m=1}^M [x_m, 1]$ where $\\mathcal{Z}$ is `fusion_tensor` and $M$ is the number of modalities and $x$ is an input from each source. Therefore, $x_m \\in \\mathbb{R}^{s_m}\\ \\forall m \\in M$ where $s_m$ is `input_size[m]` and $\\mathcal{Z} \\in \\mathbb{R}^{(s_1+1) \\times (s_2+1) \\times \\cdots \\times (s_M+1)}$  \n",
    "\n",
    "Then, the `fusion_tensor` is multiplied to a `fusion_weight` $\\mathcal{W} \\in \\mathbb{R}^{(s_1+1) \\times (s_2+1) \\times \\cdots \\times (s_M+1) \\times s_{out}}$ where $s_{out}$ is `output_size`:  \n",
    "\n",
    "$h = \\mathcal{Z} \\cdot \\mathcal{W} \\in \\mathbb{R}^{s_{out}}$ where $h$ is the `fusion_output`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43a8b50-9d5e-42d9-b4c4-683fb776010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_sizes = [64, 4, 16]\n",
    "inputs = [torch.randn((batch_size, input_size), dtype=DTYPE) for input_size in input_sizes]\n",
    "\n",
    "output_size = 10\n",
    "fusion_weight_shape = [x+1 for x in input_sizes] + [output_size]\n",
    "fusion_weight = torch.randn(fusion_weight_shape, dtype=DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "069d8d83-9beb-4fcd-b96f-1be51aa3cff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sizes: [64, 4, 16]\n",
      "Output size: 10\n",
      "Fusion weight shape: torch.Size([65, 5, 17, 10])\n"
     ]
    }
   ],
   "source": [
    "print('Input sizes: {}'.format(input_sizes))\n",
    "print('Output size: {}'.format(output_size))\n",
    "print('Fusion weight shape: {}'.format(fusion_weight.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac55560-0aff-42ac-a0ba-585d144da9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_one(inputs):\n",
    "    batch_size = inputs[0].shape[0]\n",
    "    return [torch.cat([x, torch.ones((batch_size,1), dtype=DTYPE)], dim=1) for x in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982f29d8-0acb-4103-b05a-ecbea5b347b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_fusion_layer(inputs, fusion_weight):\n",
    "    \n",
    "    concatenated_inputs = concatenate_one(inputs)\n",
    "    \n",
    "    fusion_tensor = concatenated_inputs[0]\n",
    "    for x in concatenated_inputs[1:]:\n",
    "        fusion_tensor = torch.einsum('n...,na->n...a', fusion_tensor, x)\n",
    "    \n",
    "    output = torch.einsum('n...,...o->no', fusion_tensor, fusion_weight)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28c2bc05-3587-4033-afbb-f64bcaf92561",
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_output = tensor_fusion_layer(inputs, fusion_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c982eb8-79fb-4a5b-b5c0-3c88791bbe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion output shape: torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "print('Fusion output shape: {}'.format(fusion_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f752965c-33ee-401f-8592-18572323903d",
   "metadata": {},
   "source": [
    "## CP Tensor Fusion\n",
    "\n",
    "Given that $z_m = [x_m, 1]\\ \\forall m \\in M$,\n",
    "\n",
    "$\\mathcal{Z} = \\bigotimes_{m=1}^M z_m$.\n",
    "\n",
    "The $\\mathcal{W}$ can be CP decomposed into:\n",
    "\n",
    "$[U^{(1)}, U^{(2)}, ..., U^{(M)}, U^{(out)}]$\n",
    "\n",
    "where $U^{(m)} \\in \\mathbb{R}^{s_m+1 \\times R}\\ \\forall m \\in M$ and $U^{(out)} \\in \\mathbb{R}^{s_{out} \\times R}$ are the `factors` and $R$ is `rank`\n",
    "\n",
    "We can reconstruct the `fusion_weight` by:\n",
    "\n",
    "$\\mathcal{W} = \\sum_{r=1}^R \\bigotimes_{m=1}^M U^{(m)}[:,r] \\otimes U^{(out)}[:,r]$\n",
    "\n",
    "The computational cost of tensor fusion layer can be reduced by using CP decomposition:\n",
    "\n",
    "$h = \\mathcal{Z} \\cdot \\mathcal{W} = \\bigotimes_{m=1}^M z_m \\cdot \\sum_{r=1}^R \\left( \\bigotimes_{m=1}^M U^{(m)}[:,r] \\otimes U^{(out)}[:,r] \\right) =  \\sum_{r=1}^R \\left( \\Lambda_{m=1}^M z_m \\cdot U^{(m)}[:,r] \\right) \\otimes U^{(out)}[:,r] = \\left( \\Lambda_{m=1}^M   z_m \\cdot U^{(m)} \\right) \\otimes {U^{(out)}}^\\top$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d08a551-fb78-4f01-a330-419290a62442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPTensorized, shape=[65, 5, 17, 10], tensorized_shape=[65, 5, 17, 10], rank=10)\n",
      "FactorList(\n",
      "    (factor_0): Parameter containing: [torch.DoubleTensor of size 65x10]\n",
      "    (factor_1): Parameter containing: [torch.DoubleTensor of size 5x10]\n",
      "    (factor_2): Parameter containing: [torch.DoubleTensor of size 17x10]\n",
      "    (factor_3): Parameter containing: [torch.DoubleTensor of size 10x10]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rank = 10\n",
    "fusion_weight = tltorch.TensorizedTensor.new(fusion_weight_shape, rank, factorization='CP', dtype=DTYPE)\n",
    "tltorch.tensor_init(fusion_weight)\n",
    "print(fusion_weight)\n",
    "print(fusion_weight.factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "013eac70-7288-460d-b1b7-97d5d3d03a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_tensor_fusion_layer(inputs, fusion_weight):\n",
    "    \n",
    "    concatenated_inputs = concatenate_one(inputs)\n",
    "\n",
    "    fusion_output = 1.0\n",
    "    for x, factor in zip(concatenated_inputs, fusion_weight.factors[:-1]):\n",
    "        fusion_output = fusion_output * (x @ factor)\n",
    "    fusion_output = fusion_output @ fusion_weight.factors[-1].T\n",
    "    \n",
    "    return fusion_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b3af9-1f64-473c-ab03-7f24fdbfe082",
   "metadata": {},
   "source": [
    "Let's check if it is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb589ba9-dc4b-49cd-a7a3-0526b92b1bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the outputs close enough? True\n",
      "Is CP tensor fusion faster than regular tensor fusion? False\n",
      "CP fusion time: 0.008232831954956055\n",
      "Regular fusion time: 0.006335020065307617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keonyonglee/Projects/torch/tltorch/factorized_tensors/core.py:518: UserWarning: CPTensorized, shape=[65, 5, 17, 10], tensorized_shape=[65, 5, 17, 10], rank=10) is being reconstructed into a matrix, consider operating on the decomposed form.\n",
      "  warnings.warn(f'{self} is being reconstructed into a matrix, consider operating on the decomposed form.')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "fusion_output = cp_tensor_fusion_layer(inputs, fusion_weight)\n",
    "toc = time.time()\n",
    "cp_time = toc - tic\n",
    "tic = time.time()\n",
    "fusion_output_ = tensor_fusion_layer(inputs, fusion_weight.to_matrix())\n",
    "toc = time.time()\n",
    "reg_time = toc - tic\n",
    "print('Are the outputs close enough? {}'.format(torch.allclose(fusion_output, fusion_output_)))\n",
    "print('Is CP tensor fusion faster than regular tensor fusion? {}'.format(cp_time < reg_time))\n",
    "print('CP fusion time: {}'.format(cp_time))\n",
    "print('Regular fusion time: {}'.format(reg_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5bb3f-2f13-4573-a359-e71f509aa07a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
