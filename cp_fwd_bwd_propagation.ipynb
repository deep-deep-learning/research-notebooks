{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bb3604-85c5-46a3-b54e-9ef5bb3e2cf1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CP Forward and Barward Propagation for Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06186743-f427-4fdd-9c04-33f13f565aeb",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66916f61-9264-46df-8763-b0f59f8b07be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keonyonglee/Projects/torch/tltorch/factorized_tensors/core.py:145: UserWarning: Creating a subclass of FactorizedTensor TensorizedTensor with no name.\n",
      "  warnings.warn(f'Creating a subclass of FactorizedTensor {cls.__name__} with no name.')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tltorch\n",
    "\n",
    "DTYPE = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19e38c9-67b2-45a1-8f76-72bca79584ef",
   "metadata": {},
   "source": [
    "Given a matrix\n",
    "$\\mathbf{W} \\in \\mathbb{R}^{M \\times N}$ where $M$ = `in_features` and $N$ = `out_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ff2f3ba-f0ff-49ce-b88f-679ad3e275cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 256\n",
    "out_features = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63b475-70c5-4e60-bdf9-ced48f5637e9",
   "metadata": {},
   "source": [
    "Tensorized into $\\mathcal{W} \\in \\mathbb{R}^{m_1 \\times m_2 \\times \\cdots \\times m_D \\times n_1 \\times n_2 \\times \\cdots \\times n_D}$ where $M = \\prod_{d=1}^D m_d$ and $M = \\prod_{d=1}^D m_d$ where $D$=`order` and `in_tensorized_shape` = $(m_1, m_2, ..., m_D)$ and `out_tensorized_shape` = $(n_1, n_2, ..., n_D)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae47971d-8674-4498-be90-459a7fba31f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorizing (in, out)=((256, 128)) -> (((4, 4, 16), (4, 4, 8)))\n"
     ]
    }
   ],
   "source": [
    "tensorized_shape = tltorch.utils.get_tensorized_shape(in_features, out_features, order=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee750d-5bbf-4794-ac52-77fc8a428655",
   "metadata": {},
   "source": [
    "CP decomposition of $\\mathcal{W}$ is given by:\n",
    "\n",
    "$\\mathcal{W} = \\sum_{r=1}^R \\mathbf{gm}_1[:,r] \\otimes \\mathbf{gm}_2[:,r] \\otimes \\cdots \\otimes \\mathbf{gm}_D[:,r] \\otimes \\mathbf{gn}_1[:,r] \\otimes \\mathbf{gn}_2[:,r] \\otimes \\cdots \\otimes \\mathbf{gn}_D[:,r]$ where $R$=`rank` and $\\mathbf{gm}_d \\in \\mathbb{R}^{m_d \\times R}\\ \\forall d \\in D$ are `in_factors` and $\\mathbf{gn}_d \\in \\mathbb{R}^{n_d \\times R}\\ \\forall d \\in D$ are `out_factors` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ff1768-09ca-4e25-8c68-0b4fac179d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPTensorized, shape=[256, 128], tensorized_shape=((4, 4, 16), (4, 4, 8)), rank=100)\n",
      "FactorList(\n",
      "    (factor_0): Parameter containing: [torch.DoubleTensor of size 4x100]\n",
      "    (factor_1): Parameter containing: [torch.DoubleTensor of size 4x100]\n",
      "    (factor_2): Parameter containing: [torch.DoubleTensor of size 16x100]\n",
      "    (factor_3): Parameter containing: [torch.DoubleTensor of size 4x100]\n",
      "    (factor_4): Parameter containing: [torch.DoubleTensor of size 4x100]\n",
      "    (factor_5): Parameter containing: [torch.DoubleTensor of size 8x100]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rank = 100\n",
    "cp_tensor = tltorch.TensorizedTensor.new(tensorized_shape, rank, factorization='CP', dtype=DTYPE)\n",
    "tltorch.tensor_init(cp_tensor)\n",
    "print(cp_tensor)\n",
    "print(cp_tensor.factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03c7fc-691f-412f-be66-81ec746642c7",
   "metadata": {},
   "source": [
    "From the factors we can reconstruct the tensor $\\mathcal{W}$ or the matrix $\\mathbf{W}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d0e9b31-d253-4535-af2b-028571608d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 16, 4, 4, 8])\n",
      "torch.Size([256, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keonyonglee/Projects/torch/tltorch/factorized_tensors/core.py:518: UserWarning: CPTensorized, shape=[256, 128], tensorized_shape=((4, 4, 16), (4, 4, 8)), rank=100) is being reconstructed into a matrix, consider operating on the decomposed form.\n",
      "  warnings.warn(f'{self} is being reconstructed into a matrix, consider operating on the decomposed form.')\n"
     ]
    }
   ],
   "source": [
    "print(cp_tensor.to_tensor().shape)\n",
    "print(cp_tensor.to_matrix().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0cf435-f77f-49de-8dca-11422167bedb",
   "metadata": {},
   "source": [
    "A typical linear layer involves $\\mathbf{Y=XW}$ where $\\mathbf{X} \\in \\mathbb{R}^{B \\times M}$ and $\\mathbf{Y} \\in \\mathbb{R}^{B \\times N}$ and $B$ = `batch_size`\n",
    "\n",
    "A tensorized linear layer will equivalently do $\\mathcal{Y=XW}$ where $\\mathcal{X} \\in \\mathbb{R}^{B \\times m_1 \\times m_2 \\times \\cdots \\times m_D}$ and $\\mathcal{Y} \\in \\mathbb{R}^{B \\times n_1 \\times n_2 \\times \\cdots \\times n_D}$\n",
    "\n",
    "Instead of reconstructing the tensor or the matrix we can do **factorized forward propagation** with CP factors that is given by:\n",
    "\n",
    "$\\mathcal{Y}=\\sum_{r=1}^R (\\mathcal{X} \\times_1 \\mathbf{gm}_1[:,r] \\times_2 \\mathbf{gm}_2[:,r] \\times_3 \\cdots \\times_D \\mathbf{gm}_D[:,r]) \\otimes \\mathbf{gn}_1[:,r] \\otimes \\mathbf{gn}_2[:,r] \\otimes \\cdots \\otimes \\mathbf{gn}_D[:,r]$ where $\\times_d$ is $d$-mode product which is dot product along dimension $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f7f8088-df0b-4701-befb-f63df60e8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_times_matrix_fwd(tensor, matrix):\n",
    "    \"\"\"\n",
    "    Multiplies a tensorly CP tensorized matrix and an input matrix\n",
    "    \n",
    "    X @ W\n",
    "    \"\"\"\n",
    "    \n",
    "    order = len(tensor.tensorized_shape[0])\n",
    "    saved_tensors = []\n",
    "\n",
    "    # tensorize the input\n",
    "    output = matrix.reshape((matrix.shape[0],) + tensor.tensorized_shape[0])\n",
    "    saved_tensors.append(output)\n",
    "\n",
    "    # forward propagate with input factors\n",
    "    output = torch.einsum('na...,ar->n...r', output, tensor.factors[0])\n",
    "    saved_tensors.append(output)\n",
    "    for factor in tensor.factors[1:order]:\n",
    "        output = torch.einsum('na...r,ar->n...r', output, factor)\n",
    "        saved_tensors.append(output)\n",
    "\n",
    "    # forward propagate with output factors\n",
    "    for factor in tensor.factors[order:tensor.order-1]:\n",
    "        output = torch.einsum('n...r,ar->n...ar', output, factor)\n",
    "        saved_tensors.append(output)\n",
    "    output = torch.einsum('n...r,ar->n...a', output, tensor.factors[-1])\n",
    "    \n",
    "    # vectorize the output\n",
    "    output = output.reshape((matrix.shape[0], tensor.shape[1]))\n",
    "    \n",
    "    return output, saved_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53edb368-8849-4b24-9707-9ae57a68e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_times_matrix_bwd(tensor, grad, saved_tensors):\n",
    "    '''\n",
    "    X @ W backprob\n",
    "    '''\n",
    "    \n",
    "    order = len(tensor.tensorized_shape[0])\n",
    "    factor_grads = []\n",
    "\n",
    "    # derivative of reshape\n",
    "    grad = grad.reshape((grad.shape[0],) + tensor.tensorized_shape[1])\n",
    "\n",
    "    # derivatives for 'n...r,ar->n...a'\n",
    "    factor_grads.append(torch.einsum('...a,...r->ar', grad, saved_tensors[-1]))\n",
    "    grad = torch.einsum('...a,ar->...r', grad, tensor.factors[-1])\n",
    "\n",
    "    for (factor, saved_tensor) in zip(reversed(tensor.factors[order:tensor.order-1]), \n",
    "           reversed(saved_tensors[order:tensor.order-1])):     \n",
    "        # derivatives for 'n...r,ar->n...ar'\n",
    "        factor_grads.append(torch.einsum('...ar,...r->ar', grad, saved_tensor))\n",
    "        grad = torch.einsum('...ar,ar->...r', grad, factor)\n",
    "\n",
    "    for (factor, saved_tensor) in zip(reversed(tensor.factors[1:order]), \n",
    "           reversed(saved_tensors[1:order])):\n",
    "        # derivatives for 'na...r,ar->n...r'\n",
    "        factor_grads.append(torch.einsum('n...r,na...r->ar', grad, saved_tensor))\n",
    "        grad = torch.einsum('n...r,ar->na...r', grad, factor)\n",
    "\n",
    "    # derivatives for 'na...,ar->n...r'\n",
    "    factor_grads.append(torch.einsum('n...r,na...->ar', grad, saved_tensors[0]))\n",
    "    grad = torch.einsum('n...r,ar->na...', grad, tensor.factors[0])\n",
    "\n",
    "    # derivative for reshape\n",
    "    grad = grad.reshape((saved_tensors[0].shape[0], tensor.shape[0]))\n",
    "\n",
    "    factor_grads = [x for x in reversed(factor_grads)]\n",
    "    \n",
    "    return factor_grads, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f2193-93ab-4ae3-925e-7994a2daab57",
   "metadata": {},
   "source": [
    "Let's check if the **factorized forward propagation** is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9197e409-e5df-49c3-a186-31ba01ae7972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "X = torch.randn((batch_size, in_features), dtype=DTYPE, requires_grad=True)\n",
    "W = cp_tensor.to_matrix()\n",
    "standard_fwd = X @ W\n",
    "\n",
    "with torch.no_grad():\n",
    "    factorized_fwd, saved_tensors = cp_times_matrix_fwd(cp_tensor, X)\n",
    "\n",
    "print(torch.allclose(standard_fwd, factorized_fwd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e54ca-2b1a-4986-9ee4-d4c790093737",
   "metadata": {},
   "source": [
    "\\mathbb{R}^{{C_o}_d \\times R}Let's check if the **factorized backward propagation** is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f55014c-f7ea-48db-8297-c8fa56ccad62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dy = torch.randn_like(standard_fwd)\n",
    "standard_fwd.backward(dy)\n",
    "\n",
    "with torch.no_grad():\n",
    "    factor_grad, dx = cp_times_matrix_bwd(cp_tensor, dy, saved_tensors)\n",
    "\n",
    "for i, grad in enumerate(factor_grad):\n",
    "    print(torch.allclose(cp_tensor.factors[i].grad, grad))\n",
    "\n",
    "print(torch.allclose(dx, X.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e279a-c343-44b9-a97d-df428c7f1681",
   "metadata": {},
   "source": [
    "Let's compare the number of parameters in standard and **factorized linear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c7449b7-db31-4856-a379-b50abe86c537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in CP format: 4000\n",
      "The number of parameters in matrix format: 32768\n"
     ]
    }
   ],
   "source": [
    "print('The number of parameters in CP format: {}'.format(sum([torch.numel(x) for x in cp_tensor.factors])))\n",
    "print('The number of parameters in matrix format: {}'.format(torch.numel(W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ededf-16b1-454f-b11d-e9cf92b92c36",
   "metadata": {},
   "source": [
    "# CP Forward and Backward Propagation for CONV layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8afce5-228e-4a7e-a932-97687e303f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/keonyonglee/Projects/torch/tltorch/factorized_tensors/core.py:145: UserWarning: Creating a subclass of FactorizedTensor TensorizedTensor with no name.\n",
      "  warnings.warn(f'Creating a subclass of FactorizedTensor {cls.__name__} with no name.')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tltorch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DTYPE = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dddb4d-dbaf-4bed-bc78-3c361ee9523b",
   "metadata": {},
   "source": [
    "A convolutional kernel is a 4 dimensional tensor of $\\mathcal{W} \\in \\mathbb{R}^{C_o \\times C_i \\times K_h \\times K_w}$ where $C_o$=`out_channels`, $C_i$=`in_channels`, $K_h$=`kernel_size_h`, and $K_w$=`kernel_size_w` \n",
    "\n",
    "$\\mathcal{W}$ can be decomposed into a tensor $\\mathcal{W}' \\in \\mathbb{R}^{{C_o}_1 \\times {C_o}_2 \\times \\cdots \\times {C_o}_D \\times {C_i}_1 \\times {C_i}_2 \\times \\cdots \\times {C_i}_D \\times K_h \\times K_w}$ where $C_o = \\prod_{d=1}^D {C_o}_d$ and $C_i = \\prod_{d=1}^D {C_i}_d$ and $D$=`order`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "549f1bba-a280-41ad-9981-2af023ae8c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorized shape: ((4, 8), (4, 4), (3, 3))\n"
     ]
    }
   ],
   "source": [
    "out_channels = 32\n",
    "in_channels = 16\n",
    "kernel_size_h = 3\n",
    "kernel_size_w = 3\n",
    "order = 2\n",
    "tensorized_shape = tltorch.utils.get_tensorized_shape(out_channels, in_channels, order, verbose=False) + ((kernel_size_h, kernel_size_w),)\n",
    "print('Tensorized shape: {}'.format(tensorized_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb1f3b-8d8c-45c5-a365-321242cf3b33",
   "metadata": {},
   "source": [
    "Define $\\mathcal{W}$ with CP decomposition\n",
    "\n",
    "$\\mathcal{W} = \\sum_{r=1}^R \\mathbf{gn}_1[:,r] \\otimes \\mathbf{gn}_2[:,r] \\otimes \\cdots \\otimes \\mathbf{gn}_D[:,r] \\otimes \\mathbf{gm}_1[:,r] \\otimes \\mathbf{gm}_2[:,r] \\otimes \\cdots \\otimes \\mathbf{gm}_D[:,r] \\otimes \\mathbf{kh}[:,r] \\otimes \\mathbf{kw}[:,r]$ where $R$=`rank` and $\\mathbf{gn}_d \\in \\mathbb{R}^{{C_o}_d \\times R}\\ \\forall d \\in D$ are `out_factors` and $\\mathbf{gm}_d \\in \\mathbb{R}^{{C_i}_d \\times R}\\ \\forall d \\in D$ are `in_factors` and $\\mathbf{kh} \\in \\mathbb{R}^{k_h \\times R}, \\mathbf{kw} \\in \\mathbb{R}^{k_w \\times R}$ are `kernel_factors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6b8a858-2ca9-4043-888a-0ed666178bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPTensorized, shape=[32, 16, 9], tensorized_shape=((4, 8), (4, 4), (3, 3)), rank=100)\n",
      "FactorList(\n",
      "    (factor_0): Parameter containing: [torch.DoubleTensor of size 4x100]\n",
      "    (factor_1): Parameter containing: [torch.DoubleTensor of size 8x100]\n",
      "    (factor_2): Parameter containing: [torch.DoubleTensor of size 4x100]\n",
      "    (factor_3): Parameter containing: [torch.DoubleTensor of size 4x100]\n",
      "    (factor_4): Parameter containing: [torch.DoubleTensor of size 3x100]\n",
      "    (factor_5): Parameter containing: [torch.DoubleTensor of size 3x100]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rank = 100\n",
    "tensor = tltorch.TensorizedTensor.new(tensorized_shape, rank, factorization='CP', dtype=DTYPE)\n",
    "tltorch.tensor_init(tensor)\n",
    "print(tensor)\n",
    "print(tensor.factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c23af4-0622-4bf0-9465-02d404251a49",
   "metadata": {},
   "source": [
    "Given an input tensor $\\mathcal{X} \\in \\mathbb{R}^{C_i \\times H \\times W}$, its convolution with $\\mathcal{W}$ is given by:\n",
    "\n",
    "$\\mathcal{Y}(o,h,w) = \\sum_{i=1}^{C_i} \\sum_{k_h=1}^{K_h} \\sum_{k_w=1}^{K_w} \\mathcal{W}[k_h,k_w,i,o] \\mathcal{X}[i,h-k_h,w-h_w]\\ \\forall o \\in C_o,\\ \\forall h \\in H,\\ \\forall w \\in W$\n",
    "\n",
    "Its **factorized forward propagation** is given by:\n",
    "\n",
    "$\\mathcal{Y}(o_1,o_2,...,o_D,h,w) = \\sum_{r=1}^R \\left(\\sum_{k_h=1}^{K_h} \\sum_{k_w=1}^{K_w} \\left(\\mathcal{X}' \\times_1 \\mathbf{gm}_1[:,r] \\times_2 \\mathbf{gm}_2[:,r] \\times_3 \\cdots \\times_D \\mathbf{gm}_D[:,r]\\right)[h-k_h,w-h_w]\\right) \\otimes \\mathbf{gn}_1[:,r] \\otimes \\mathbf{gn}_2[:,r] \\otimes \\cdots \\otimes \\mathbf{gn}_D[:,r]$ \n",
    "\n",
    "where $\\mathcal{X}' \\in \\mathbb{R}^{{C_i}_1 \\times {C_i}_2 \\times \\cdots {C_i}_D \\times H \\times W}$ is a reshaped $\\mathcal{X}$\n",
    "\n",
    "This can be seen as:\n",
    "1. forward propagate with `in_factors`\n",
    "2. convolve with `kernel_factors`\n",
    "3. forward propagate with `out_factors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b909e01-5435-4d86-93b7-72e44b1cb4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_conv_fwd(tensor, input_tensor, order):\n",
    "    \n",
    "    saved_tensors = []\n",
    "    \n",
    "    # tensorize the input\n",
    "    output = input_tensor.reshape((input_tensor.shape[0],) + tensor.tensorized_shape[1] + input_tensor.shape[-2:])\n",
    "    saved_tensors.append(output)\n",
    "    \n",
    "    # forward propagate with input factors\n",
    "    output = torch.einsum('na...xy,ar->nr...xy', output, tensor.factors[order])\n",
    "    saved_tensors.append(output)\n",
    "    for factor in tensor.factors[order+1:-2]:\n",
    "        output = torch.einsum('nra...xy,ar->nr...xy', output, factor)\n",
    "        saved_tensors.append(output)\n",
    "    \n",
    "    # x and y convolutions\n",
    "    output = torch.nn.functional.conv2d(output, \n",
    "                                        tensor.factors[-2].T.reshape(tensor.rank, 1, tensor.tensorized_shape[-1][0], 1), \n",
    "                                        padding='same', \n",
    "                                        groups=tensor.rank)\n",
    "    saved_tensors.append(output)\n",
    "    \n",
    "    output = torch.nn.functional.conv2d(output, \n",
    "                                        tensor.factors[-1].T.reshape(tensor.rank, 1, 1, tensor.tensorized_shape[-1][1]), \n",
    "                                        padding='same', \n",
    "                                        groups=tensor.rank)\n",
    "    saved_tensors.append(output)\n",
    "    # forward propagate with output factors\n",
    "    for factor in tensor.factors[:order-1]:\n",
    "        output = torch.einsum('nr...xy,ar->nr...axy', output, factor)\n",
    "        saved_tensors.append(output)\n",
    "    output = torch.einsum('nr...xy,ar->n...axy', output, tensor.factors[order-1])\n",
    "\n",
    "    # reshape the output\n",
    "    output = output.reshape((output.shape[0], tensor.shape[0], output.shape[-2], output.shape[-1]))\n",
    "    \n",
    "    return output, saved_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b4d01c2-f935-4844-9b14-21dca0e8f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_conv_bwd(tensor, dy, saved_tensors):\n",
    "    \n",
    "    order = len(tensor.tensorized_shape[0])\n",
    "    \n",
    "    out_factor_grads = []\n",
    "    \n",
    "    dy = dy.reshape((saved_tensors[0].shape[0],) + tensor.tensorized_shape[0] + saved_tensors[0].shape[-2:])\n",
    "\n",
    "    out_factor_grads.append(torch.einsum('n...axy,nr...xy->ar', dy, saved_tensors[-1]))\n",
    "    dy = torch.einsum('n...axy,ar->nr...xy', dy, tensor.factors[order-1])\n",
    "    \n",
    "    for factor, saved_tensor in zip(reversed(tensor.factors[:order-1]), \n",
    "                                    reversed(saved_tensors[-order:-1])):\n",
    "        out_factor_grads.append(torch.einsum('nr...axy,nr...xy->ar', dy, saved_tensor))\n",
    "        dy = torch.einsum('nr...axy,ar->nr...xy', dy, factor)\n",
    "\n",
    "    \n",
    "    factor_grads = []\n",
    "    pad = tensor.tensorized_shape[-1][1]//2\n",
    "    factor_grads.append(F.conv3d(torch.einsum('ncxy->cnxy', saved_tensors[-order-1]).unsqueeze(0), \n",
    "                                 torch.einsum('ncxy->cnxy', dy).unsqueeze(1), \n",
    "                                 padding=(0,0,pad), \n",
    "                                 groups=tensor.rank).squeeze(0).reshape(tensor.rank, tensor.tensorized_shape[-1][1]).T)\n",
    "    dy = torch.nn.functional.conv_transpose2d(dy, \n",
    "                                              tensor.factors[-1].T.reshape(tensor.rank, 1, 1, tensor.tensorized_shape[-1][1]),\n",
    "                                              padding=(0,pad), \n",
    "                                              groups=tensor.rank)\n",
    "    \n",
    "\n",
    "    pad = tensor.tensorized_shape[-1][0]//2\n",
    "    factor_grads.append(F.conv3d(torch.einsum('ncxy->cnxy', saved_tensors[-order-2]).unsqueeze(0), \n",
    "                                 torch.einsum('ncxy->cnxy', dy).unsqueeze(1), \n",
    "                                 padding=(0,pad,0), \n",
    "                                 groups=tensor.rank).squeeze(0).reshape(tensor.rank, tensor.tensorized_shape[-1][0]).T)\n",
    "    dy = torch.nn.functional.conv_transpose2d(dy, \n",
    "                                              tensor.factors[-2].T.reshape(tensor.rank, 1, tensor.tensorized_shape[-1][0], 1),\n",
    "                                              padding=(pad,0), \n",
    "                                              groups=tensor.rank)\n",
    "\n",
    "    for factor, saved_tensor in zip(reversed(tensor.factors[order+1:order*2]), \n",
    "                                    reversed(saved_tensors[1:-order-2])):\n",
    "        factor_grads.append(torch.einsum('nr...xy,nra...xy->ar', dy, saved_tensor))\n",
    "        dy = torch.einsum('nr...xy,ar->nra...xy', dy, factor)\n",
    "\n",
    "    factor_grads.append(torch.einsum('nr...xy,na...xy->ar', dy, saved_tensors[0]))\n",
    "    dy = torch.einsum('nr...xy,ar->na...xy', dy, tensor.factors[order])\n",
    "    \n",
    "    dy = dy.reshape((saved_tensors[0].shape[0], tensor.shape[1]) + saved_tensors[0].shape[-2:])\n",
    "    \n",
    "    factor_grads = [x for x in reversed(out_factor_grads)] + [x for x in reversed(factor_grads)]\n",
    "    \n",
    "    return factor_grads, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf40f8-61ca-41a9-97d4-f1c17441f0a7",
   "metadata": {},
   "source": [
    "Let's check if the **factorized forward propagation** is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8747f8e6-56c7-49db-981f-dbbb392d55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "x = 32\n",
    "y = 32\n",
    "input_tensor = torch.randn((batch_size, in_channels, x, y), dtype=DTYPE, requires_grad=True)\n",
    "\n",
    "kernel = tensor.to_matrix().reshape(out_channels, in_channels, kernel_size_x, kernel_size_y)\n",
    "standard_fwd = F.conv2d(input_tensor, kernel, bias=None, padding='same')\n",
    "\n",
    "with torch.no_grad():\n",
    "    factorized_fwd, saved_tensors = cp_conv_fwd(tensor, input_tensor, 2)\n",
    "\n",
    "print(torch.allclose(standard_fwd, factorized_fwd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3641ed12-e53a-4249-a121-0b3d9aba02b9",
   "metadata": {},
   "source": [
    "Let's check if the **factorized backward propagation** is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9869e3c1-e128-4406-8457-636ff75f505f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dy = torch.randn_like(standard_fwd)\n",
    "standard_fwd.backward(dy)\n",
    "\n",
    "factor_grads, dx = cp_conv_bwd(tensor, dy, saved_tensors)\n",
    "for i, grad in enumerate(factor_grads):\n",
    "    print(torch.allclose(tensor.factors[i].grad, grad))\n",
    "print(torch.allclose(dx, input_tensor.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a59219-c204-4745-9d63-84a634031703",
   "metadata": {},
   "source": [
    "Let's compare the number of parameters in standard and **factorized linear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a37e59cb-4cf5-4887-94f5-433cc398c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in CP format: 2600\n",
      "The number of parameters in matrix format: 4608\n"
     ]
    }
   ],
   "source": [
    "print('The number of parameters in CP format: {}'.format(sum([torch.numel(x) for x in tensor.factors])))\n",
    "print('The number of parameters in matrix format: {}'.format(torch.numel(kernel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93c3c7-47e7-464c-b382-1fd63368b932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
